{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY6s9x8fz0vu4zYmdV/vgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VickyKandale/Assignment_pyhton.pwskills/blob/main/Assignment_20th_Apr_(KNN_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN-1\n"
      ],
      "metadata": {
        "id": "9p2FyxqE_geU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty8MOhzj99H8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is the KNN algorithm?"
      ],
      "metadata": {
        "id": "IS1PqgVd_fEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm is a simple and popular machine learning algorithm used for both classification and regression tasks. It is a type of instance-based learning or lazy learning method, where the model does not explicitly learn a function during the training phase. Instead, it memorizes the entire training dataset and makes predictions based on the similarity between new data points and the existing data points in the training set.\n",
        "\n",
        "In KNN, the \"K\" represents the number of nearest neighbors that are considered when making predictions for a new data point. The basic idea behind KNN is to find the K data points in the training set that are closest to the new data point based on some distance metric (commonly Euclidean distance). The algorithm then assigns the class label (for classification) or calculates the average (for regression) of the K nearest neighbors to determine the prediction for the new data point.\n",
        "\n",
        "Here's a high-level overview of the KNN algorithm:\n",
        "\n",
        "Training Phase: During the training phase, KNN memorizes the entire training dataset without performing any explicit model training or parameter estimation. This makes the training phase relatively fast and memory-intensive.\n",
        "\n",
        "Prediction Phase: For each new data point in the test set, the algorithm calculates the distance between the new point and all data points in the training set. The K data points with the smallest distances to the new point are selected as the nearest neighbors.\n",
        "\n",
        "Classification: For classification tasks, the most common class among the K nearest neighbors is assigned as the predicted class for the new data point.\n",
        "\n",
        "Regression: For regression tasks, the algorithm takes the average (or weighted average) of the target values of the K nearest neighbors and assigns it as the predicted value for the new data point.\n",
        "\n",
        "Choosing K: The value of K is a hyperparameter that needs to be set before training the model. It can be determined through hyperparameter tuning techniques such as cross-validation.\n",
        "\n",
        "KNN is a simple and intuitive algorithm, but it can be sensitive to the choice of K and may not perform well with high-dimensional data or imbalanced datasets. Despite its limitations, KNN is often used as a baseline model for classification and regression tasks due to its ease of implementation and interpretability."
      ],
      "metadata": {
        "id": "79lnuTPnMEHq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibwDg4uv_f6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. How do you choose the value of K in KNN?"
      ],
      "metadata": {
        "id": "DJK19ANx_tvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is an important hyperparameter tuning step that can significantly impact the performance of the model. The appropriate choice of K depends on the nature of the data and the specific problem you are trying to solve. There are several methods to choose the value of K:\n",
        "\n",
        "Brute Force Search: One approach is to perform a brute force search over a range of K values and evaluate the model's performance (e.g., using cross-validation) for each K. You can plot the performance metric (such as accuracy for classification or mean squared error for regression) against different K values and choose the one that gives the best performance.\n",
        "\n",
        "Rule of Thumb: A common rule of thumb is to choose K as the square root of the number of data points in the training set. For example, if you have 100 data points, you might start with K=10 (since âˆš100=10). However, this rule may not always be optimal and should be used as an initial estimate.\n",
        "\n",
        "Odd vs. Even K: When dealing with binary classification problems, it is often recommended to choose an odd value for K to avoid ties in voting. Ties can occur when two classes have an equal number of votes, leading to an ambiguous prediction.\n",
        "\n",
        "Cross-Validation: Cross-validation is a powerful technique to assess the model's performance on different K values. You can use k-fold cross-validation to split the training data into multiple subsets, train the model on various K values, and then average the performance metrics to get a more robust estimate of the model's accuracy for each K.\n",
        "\n",
        "Domain Knowledge: Depending on the specific problem and the underlying nature of the data, you might have prior knowledge about the expected number of neighbors that are meaningful for predictions. Incorporating domain knowledge can guide you in selecting an appropriate value for K.\n",
        "\n",
        "Grid Search / Random Search: If you have a large dataset and want to automate the process of selecting K, you can use hyperparameter tuning techniques like grid search or random search to find the best K value based on the chosen evaluation metric.\n",
        "\n",
        "Ultimately, the best choice of K should be based on empirical evaluation using validation techniques like cross-validation and should take into account the specific characteristics of your dataset and the problem you are trying to solve. It's important to note that selecting K is not a one-size-fits-all decision, and it may require some experimentation to find the optimal value for your specific use case."
      ],
      "metadata": {
        "id": "ddtS1-d-M3xI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYDeoM0w_vaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. What is the difference between KNN classifier and KNN regressor?"
      ],
      "metadata": {
        "id": "FVD-QaK-MIR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between K-Nearest Neighbors (KNN) classifier and KNN regressor lies in their application and the type of output they provide:\n",
        "\n",
        "Application:\n",
        "\n",
        "KNN Classifier: KNN classifier is used for classification tasks, where the goal is to predict the class or category of a data point based on its neighbors' class labels. For example, it can be used to classify whether an email is spam or not, or to identify the species of a plant based on its features.\n",
        "KNN Regressor: KNN regressor, on the other hand, is used for regression tasks, where the goal is to predict a continuous numeric value for a given data point. For example, it can be used to predict the house price based on its features, or to estimate the temperature based on historical weather data.\n",
        "Output:\n",
        "\n",
        "KNN Classifier: The output of the KNN classifier is the most common class label among the K nearest neighbors of the query data point. In the case of ties, the algorithm can choose the class label based on various strategies (e.g., selecting the majority class among the K nearest neighbors or using weighted voting).\n",
        "KNN Regressor: The output of the KNN regressor is the average (or weighted average) of the target values of the K nearest neighbors. It provides a continuous numeric value as the prediction.\n",
        "Evaluation Metrics:\n",
        "\n",
        "KNN Classifier: The performance of a KNN classifier is typically evaluated using metrics such as accuracy, precision, recall, F1-score, or the confusion matrix, depending on the specific problem and the class distribution.\n",
        "KNN Regressor: The performance of a KNN regressor is commonly evaluated using metrics such as mean squared error (MSE), mean absolute error (MAE), or R-squared (coefficient of determination).\n",
        "Distance Metrics:\n",
        "\n",
        "KNN Classifier and KNN Regressor both rely on distance metrics (e.g., Euclidean distance) to measure the similarity between data points and find the K nearest neighbors.\n",
        "Despite their differences, both KNN classifier and KNN regressor are non-parametric algorithms and belong to the same family of KNN algorithms. They share the same underlying principles of identifying the K nearest neighbors based on distance metrics and making predictions based on the neighbors' information. The choice between KNN classifier and KNN regressor depends on the nature of the problem and the type of output required."
      ],
      "metadata": {
        "id": "O0wtjSchNPIx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pantEv7RMJU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. How do you measure the performance of KNN?"
      ],
      "metadata": {
        "id": "BOBRF9U7Mrba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of the K-Nearest Neighbors (KNN) algorithm can be measured using various evaluation metrics, depending on whether the KNN is applied for classification or regression tasks. Here are some common performance metrics for each case:\n",
        "\n",
        "1. KNN for Classification:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances over the total number of instances in the test set.\n",
        "Precision: The number of true positive predictions divided by the total number of positive predictions (TP / (TP + FP)).\n",
        "Recall (Sensitivity or True Positive Rate): The number of true positive predictions divided by the total number of actual positive instances (TP / (TP + FN)).\n",
        "F1-score: The harmonic mean of precision and recall, providing a balance between the two metrics (2 * (Precision * Recall) / (Precision + Recall)).\n",
        "Confusion Matrix: A table that shows the true positive, false positive, true negative, and false negative predictions, providing a detailed view of the model's performance across classes.\n",
        "2. KNN for Regression:\n",
        "\n",
        "Mean Squared Error (MSE): The average of the squared differences between the predicted values and the true target values. It penalizes larger errors more heavily.\n",
        "Mean Absolute Error (MAE): The average of the absolute differences between the predicted values and the true target values. It provides a measure of the magnitude of errors without considering their direction.\n",
        "R-squared (Coefficient of Determination): The proportion of the variance in the target variable that is explained by the model. It measures how well the model fits the data compared to a simple mean-based model.\n",
        "Cross-Validation: In addition to the above metrics, it is common practice to perform cross-validation (e.g., k-fold cross-validation) to get a more robust estimate of the KNN model's performance. Cross-validation helps in assessing how well the model generalizes to unseen data and reduces the risk of overfitting."
      ],
      "metadata": {
        "id": "DHmq33M7NY-f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEc_fa-PMsKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. What is the curse of dimensionality in KNN?"
      ],
      "metadata": {
        "id": "Fe51s6irM_cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"curse of dimensionality\" is a term used to describe the challenges and issues that arise when dealing with high-dimensional data in machine learning, particularly in the context of algorithms like K-Nearest Neighbors (KNN). It refers to the fact that as the number of features (dimensions) in the dataset increases, the data points become increasingly sparse, and the behavior of the data in high-dimensional space becomes counterintuitive.\n",
        "\n",
        "In the case of KNN, the curse of dimensionality manifests in several ways:\n",
        "\n",
        "Increased Distance: As the number of dimensions increases, the distance between data points becomes less informative. In high-dimensional space, most points are almost equidistant from each other, making it challenging to find meaningful neighbors.\n",
        "\n",
        "Increased Computational Complexity: As the dimensionality of the data increases, the number of pairwise distances to compute also increases significantly. This can lead to a substantial increase in computational time and memory requirements for KNN, making it computationally expensive for high-dimensional datasets.\n",
        "\n",
        "Data Sparsity: In high-dimensional space, the data points tend to become more dispersed, and the volume of the space increases exponentially with the number of dimensions. As a result, the available data may become sparse, and the risk of overfitting the model to noise in the data also increases.\n",
        "\n",
        "Increased Number of Irrelevant Features: In high-dimensional data, many features may be irrelevant or not contribute significantly to the prediction task. These irrelevant features can introduce noise and negatively impact the model's performance.\n",
        "\n",
        "Curse of Localness: In low-dimensional space, nearest neighbors often share more similarities with each other. However, as the dimensionality increases, the notion of proximity becomes less meaningful, and KNN may start considering distant data points as neighbors, leading to poorer performance.\n",
        "\n",
        "To mitigate the curse of dimensionality in KNN and other high-dimensional data scenarios, feature selection or dimensionality reduction techniques can be employed to reduce the number of features while retaining relevant information. Techniques like Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and feature engineering can be useful to reduce the data's dimensionality and improve the performance of KNN and other algorithms in such scenarios."
      ],
      "metadata": {
        "id": "HyYr-XzTPsRm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWg884MbNAs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. How do you handle missing values in KNN?"
      ],
      "metadata": {
        "id": "EdqCiXbVNboo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing values in the K-Nearest Neighbors (KNN) algorithm is important to ensure accurate and reliable predictions. Since KNN relies on the distances between data points, having missing values can significantly impact the algorithm's performance. Here are some common approaches to handle missing values in KNN:\n",
        "\n",
        "Removing Data Points: One straightforward approach is to remove data points that contain missing values. However, this should be done carefully, as removing too many data points can lead to a loss of valuable information and potentially bias the model.\n",
        "\n",
        "Imputation with Mean, Median, or Mode: For numerical features, you can impute missing values with the mean, median, or mode of the non-missing values of that feature. This approach fills in the missing values with central tendencies, which can be a reasonable approximation if the missing values are not strongly correlated with other features.\n",
        "\n",
        "Imputation with KNN: A more advanced approach is to use KNN itself for imputation. For each data point with missing values, you can identify its K nearest neighbors based on the non-missing features and use their values to impute the missing values. The average (for numerical features) or majority vote (for categorical features) of the neighbors' values can be used for imputation.\n",
        "\n",
        "Imputation with Regression: If the feature with missing values is numerical and has a strong correlation with other features, you can use regression techniques to predict the missing values based on the other features.\n",
        "\n",
        "Multiple Imputation: For more sophisticated handling of missing values, you can use multiple imputation techniques to create multiple imputed datasets and then perform KNN on each imputed dataset. The results from multiple imputations can be combined to obtain more robust predictions.\n",
        "\n",
        "Feature Engineering: If the feature with missing values is categorical, you can create a new category for the missing values or encode the missing values as a separate category."
      ],
      "metadata": {
        "id": "BubHQh0dQQyA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLQEmDyJNdIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "\n"
      ],
      "metadata": {
        "id": "LoydZ80EPvSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of the K-Nearest Neighbors (KNN) classifier and regressor can vary based on the nature of the problem and the characteristics of the dataset. Let's compare and contrast the performance of both approaches:\n",
        "\n",
        "1. KNN Classifier:\n",
        "\n",
        "Application: KNN classifier is used for classification tasks, where the goal is to predict the class or category of a data point based on its neighbors' class labels.\n",
        "Output: The output of the KNN classifier is the most common class among the K nearest neighbors of the query data point.\n",
        "Evaluation Metrics: The performance of the KNN classifier is typically evaluated using metrics such as accuracy, precision, recall, F1-score, and the confusion matrix.\n",
        "Use Cases: KNN classifier is suitable for problems with discrete class labels, such as spam detection, sentiment analysis, image classification, and medical diagnosis.\n",
        "2. KNN Regressor:\n",
        "\n",
        "Application: KNN regressor is used for regression tasks, where the goal is to predict a continuous numeric value for a given data point.\n",
        "Output: The output of the KNN regressor is the average (or weighted average) of the target values of the K nearest neighbors.\n",
        "Evaluation Metrics: The performance of the KNN regressor is commonly evaluated using metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared (coefficient of determination).\n",
        "Use Cases: KNN regressor is suitable for problems with continuous target variables, such as house price prediction, stock market forecasting, and weather temperature estimation.\n",
        "Comparison:\n",
        "\n",
        "Similarities: Both KNN classifier and regressor rely on the same underlying principle of finding the K nearest neighbors based on distance metrics to make predictions.\n",
        "Differences: The main difference lies in the type of output they provide and the evaluation metrics used to measure their performance.\n",
        "Which One is Better for Which Type of Problem?\n",
        "\n",
        "KNN Classifier: It is better suited for problems with discrete and categorical target variables. For example, if you want to classify data into different classes or categories, KNN classifier can be a good choice.\n",
        "KNN Regressor: It is better suited for problems with continuous target variables. If the goal is to predict a numeric value, KNN regressor can be more appropriate."
      ],
      "metadata": {
        "id": "83bH55l4QXPt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tr2Ual9PyZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
      ],
      "metadata": {
        "id": "F2S0d7HIQKqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-Nearest Neighbors (KNN) algorithm has its strengths and weaknesses for both classification and regression tasks. Understanding these aspects can help in making informed decisions and addressing potential challenges. Let's explore the strengths and weaknesses of KNN for each type of task and how to address them:\n",
        "\n",
        "Strengths of KNN:\n",
        "\n",
        "Simple and Intuitive: KNN is easy to understand and implement. It doesn't involve complex mathematical calculations or model training, making it an attractive choice for beginners.\n",
        "\n",
        "Non-Parametric: KNN is a non-parametric algorithm, which means it doesn't make assumptions about the underlying data distribution. It can handle complex relationships and adapt well to different types of data.\n",
        "\n",
        "Adaptability to Data: KNN can handle both linear and non-linear relationships in the data, making it suitable for various types of datasets.\n",
        "\n",
        "Efficient for Small Datasets: KNN's training phase is fast since it memorizes the entire training dataset. It is particularly efficient for small datasets.\n",
        "\n",
        "Robust to Outliers: KNN is less affected by outliers since it considers the nearest neighbors, which are less likely to be influenced by extreme values.\n",
        "\n",
        "Weaknesses of KNN:\n",
        "\n",
        "Computationally Intensive: KNN's prediction phase can be computationally expensive, especially for large datasets, as it requires calculating distances between the query point and all data points in the training set.\n",
        "\n",
        "Sensitivity to Data Scaling: KNN is sensitive to the scale of features. If the features have different scales, features with larger magnitudes can dominate the distance calculations, leading to biased predictions.\n",
        "\n",
        "Curse of Dimensionality: KNN's performance can degrade in high-dimensional spaces due to the curse of dimensionality. As the number of features increases, the number of neighbors needed to make reliable predictions grows exponentially.\n",
        "\n",
        "Imbalanced Data: KNN may not perform well with imbalanced datasets, where one class heavily outweighs the others. The majority class may dominate the prediction for the minority class.\n",
        "\n",
        "Addressing the Weaknesses:\n",
        "\n",
        "Data Scaling: Feature scaling, such as normalization or standardization, can address the sensitivity to data scaling. Scaling the features to a similar range can improve the performance of KNN.\n",
        "\n",
        "Dimensionality Reduction: Dimensionality reduction techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE) can help mitigate the curse of dimensionality by reducing the number of features while retaining the essential information.\n",
        "\n",
        "Efficient Distance Metrics: Using efficient distance metrics, like the cosine similarity for text data or Manhattan distance for sparse data, can speed up the computation for large datasets.\n",
        "\n",
        "Handling Imbalanced Data: Techniques like oversampling the minority class, undersampling the majority class, or using class weights can help address imbalanced data issues in KNN."
      ],
      "metadata": {
        "id": "88Td_mBZQe4n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5O3SuYl_QMs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
      ],
      "metadata": {
        "id": "kB1FChQcQa1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Euclidean distance and Manhattan distance are two common distance metrics used in the K-Nearest Neighbors (KNN) algorithm to measure the similarity between data points. They are used to determine the nearest neighbors in the KNN algorithm based on their proximity to the query point. The main difference between the two metrics lies in how they calculate the distance between two points in a multi-dimensional space:\n",
        "\n",
        "Euclidean Distance:\n",
        "\n",
        "Euclidean distance is the straight-line distance between two points in a Euclidean space (i.e., a space with a fixed number of dimensions).\n",
        "In a 2D space, the Euclidean distance between two points (x1, y1) and (x2, y2) is given by: âˆš((x2 - x1)^2 + (y2 - y1)^2)\n",
        "In a 3D space, the Euclidean distance between two points (x1, y1, z1) and (x2, y2, z2) is given by: âˆš((x2 - x1)^2 + (y2 - y1)^2 + (z2 - z1)^2)\n",
        "The Euclidean distance is sensitive to the scale of features and gives more weight to features with larger magnitudes.\n",
        "Manhattan Distance:\n",
        "\n",
        "Manhattan distance, also known as city block distance or L1 distance, is the sum of the absolute differences between the coordinates of two points.\n",
        "In a 2D space, the Manhattan distance between two points (x1, y1) and (x2, y2) is given by: |x2 - x1| + |y2 - y1|\n",
        "In a 3D space, the Manhattan distance between two points (x1, y1, z1) and (x2, y2, z2) is given by: |x2 - x1| + |y2 - y1| + |z2 - z1|\n",
        "The Manhattan distance is less sensitive to the scale of features and is often preferred when dealing with data that has different units or scales.\n",
        "Comparison:\n",
        "\n",
        "Both Euclidean distance and Manhattan distance are valid distance metrics used in KNN, and the choice between them depends on the specific characteristics of the data and the problem at hand.\n",
        "Euclidean distance works well when the features have similar scales and the data follows a more isotropic distribution (similar spread in all dimensions).\n",
        "Manhattan distance can be more appropriate when dealing with data with different units or scales, as it is less affected by feature scaling."
      ],
      "metadata": {
        "id": "t0YV0c5dQnYV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcDhd8NKQbok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. What is the role of feature scaling in KNN?"
      ],
      "metadata": {
        "id": "7m7WhYofQh_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The role of feature scaling in K-Nearest Neighbors (KNN) is crucial for ensuring fair and accurate distance calculations between data points. Feature scaling is the process of transforming the features of the dataset to a similar scale or range. It helps in normalizing the data and preventing certain features from dominating the distance computations, which can influence the KNN algorithm's results. The main reasons to use feature scaling in KNN are as follows:\n",
        "\n",
        "Equalizing Feature Influence: In KNN, the similarity between data points is determined by calculating distances between their feature values. Features with larger magnitudes can dominate the distance calculations, giving them more influence over the nearest neighbors selection. Feature scaling ensures that all features have equal importance in determining the similarity between data points.\n",
        "\n",
        "Handling Different Scales: If features have different units or scales, it can lead to discrepancies in distance computations. For example, a feature measured in kilometers might have a much larger numerical range compared to a feature measured in centimeters. This discrepancy can distort the distance metric and influence the results of the KNN algorithm.\n",
        "\n",
        "Improving Convergence: Feature scaling can improve the convergence rate of the KNN algorithm. By normalizing the data, it helps in finding the correct nearest neighbors more efficiently and can lead to faster convergence during the prediction phase.\n",
        "\n",
        "Handling High-Dimensional Data: In high-dimensional datasets, the distance between data points becomes less meaningful due to the curse of dimensionality. Feature scaling can help mitigate this issue and improve the performance of KNN in high-dimensional spaces.\n",
        "\n",
        "Common methods for feature scaling in KNN include:\n",
        "\n",
        "Normalization (Min-Max Scaling): Scales features to a range between 0 and 1 by subtracting the minimum value and dividing by the range (max-min).\n",
        "Standardization (Z-Score Scaling): Scales features to have a mean of 0 and a standard deviation of 1 by subtracting the mean and dividing by the standard deviation.\n",
        "Robust Scaling: Scales features using median and interquartile range, making it more robust to outliers."
      ],
      "metadata": {
        "id": "fw8I9tGCQq01"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3SCEYSkQi4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}