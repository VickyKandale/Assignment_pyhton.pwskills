{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7zVUCKDqSCWBnf2dSyTYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VickyKandale/Assignment_pyhton.pwskills/blob/main/Assignment_(Anomaly_Detection_1)_2nd_may.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection-1"
      ],
      "metadata": {
        "id": "-cUu_aq73W7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is anomaly detection and what is its purpose?"
      ],
      "metadata": {
        "id": "voctoRAR3fbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection, also known as outlier detection, is a technique used to identify patterns or instances that significantly deviate from the norm or expected behavior within a dataset. These anomalous patterns or instances are often referred to as anomalies or outliers.\n",
        "\n",
        "The purpose of anomaly detection is to identify unusual, rare, or suspicious observations that do not conform to the typical patterns or behavior of the majority of the data. Anomalies can occur due to various reasons, such as errors in data collection, fraudulent activities, system faults, or rare events that have different characteristics from the regular data.\n",
        "\n",
        "The key objectives of anomaly detection are:\n",
        "\n",
        "- Identifying Unusual Patterns: Anomaly detection aims to distinguish anomalies from the normal patterns or behaviors in a dataset. By detecting these unusual patterns, it helps in understanding the underlying causes or factors contributing to the anomalies.\n",
        "\n",
        "- Early Warning and Fault Detection: Anomaly detection is employed in many fields, including finance, cybersecurity, manufacturing, and healthcare, to provide early warning signals for potential issues or faults. By identifying anomalies in real-time or near-real-time, it enables proactive measures to be taken before significant problems occur.\n",
        "\n",
        "- Fraud and Intrusion Detection: Anomaly detection is widely used for detecting fraudulent activities or intrusions in various domains. It helps in identifying suspicious transactions, fraudulent behavior, network intrusions, or system attacks that deviate from the normal usage or expected patterns.\n",
        "\n",
        "- Data Cleaning and Quality Assurance: Anomaly detection can be utilized to identify errors, noise, or outliers in datasets, aiding in data cleaning and quality assurance processes. By identifying and handling outliers, it helps in improving the accuracy and reliability of subsequent data analysis or modeling tasks.\n",
        "\n",
        "- Novelty Detection: Anomaly detection can also be employed to identify novel or previously unseen patterns in the data. This can be useful for discovering new insights, emerging trends, or previously unknown phenomena.\n",
        "\n",
        "Overall, the purpose of anomaly detection is to flag and investigate instances or patterns that are significantly different from the norm. By identifying anomalies, it enables organizations to take appropriate actions, prevent potential issues, enhance data quality, and ensure the integrity and security of systems and processes."
      ],
      "metadata": {
        "id": "jvnAioKx3toS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bp4P79W3PBU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What are the key challenges in anomaly detection?"
      ],
      "metadata": {
        "id": "qhDp8Lrl3g1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is the task of identifying data points that deviate from the norm. This can be a challenging task, as there are a number of factors that can contribute to anomalies, including noise, outliers, and concept drift.\n",
        "\n",
        "Here are some of the key challenges in anomaly detection:\n",
        "\n",
        "- Defining what constitutes an anomaly. This can be difficult, as what is considered an anomaly in one context may not be considered an anomaly in another. For example, a sudden spike in traffic on a website may be considered an anomaly if it is outside of the normal range, but it may not be considered an anomaly if it is caused by a planned marketing campaign.\n",
        "- Obtaining enough data. Anomaly detection models typically require a large amount of data in order to learn the normal behavior of the data. If the amount of data is too small, the model may not be able to learn the normal behavior and may identify too many false positives.\n",
        "- Dealing with noise. Noise is a common problem in data sets, and it can make it difficult to identify anomalies. Noise can be caused by a number of factors, such as measurement errors, outliers, and random fluctuations.\n",
        "- Concept drift. Concept drift is the phenomenon where the underlying distribution of the data changes over time. This can make it difficult to identify anomalies, as the model may not be able to keep up with the changes in the data.\n",
        "- Despite these challenges, anomaly detection is a powerful tool that can be used to identify potential problems in a variety of domains. By addressing the challenges of anomaly detection, it is possible to develop models that can accurately identify anomalies and help to prevent problems before they occur.\n",
        "\n",
        "Here are some additional challenges in anomaly detection:\n",
        "\n",
        "- Selecting the right anomaly detection algorithm. There are a number of different anomaly detection algorithms available, and each algorithm has its own strengths and weaknesses. The choice of algorithm will depend on the - specific application and the characteristics of the data.\n",
        "- Tuning the anomaly detection parameters. The performance of an anomaly detection model can be sensitive to the values of the parameters. The parameters need to be tuned carefully in order to achieve the desired level of performance.\n",
        "- Deploying the anomaly detection model. Once an anomaly detection model has been developed, it needs to be deployed in a production environment. This can be a challenge, as it requires the model to be integrated with the existing infrastructure and the model's predictions need to be communicated to the stakeholders.\n",
        "\n",
        "By addressing these challenges, it is possible to develop and deploy anomaly detection models that can effectively identify anomalies and help to prevent problems before they occur."
      ],
      "metadata": {
        "id": "KTohrIKm33mq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gMhcJD3V3jEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
      ],
      "metadata": {
        "id": "lSE6_i_H4Dnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised anomaly detection and supervised anomaly detection are two approaches used to identify anomalies in data, and they differ in terms of their reliance on labeled or unlabeled data. Here's a comparison of the two approaches:\n",
        "\n",
        "1. Training Data Requirement:\n",
        "\n",
        "- Unsupervised Anomaly Detection: Unsupervised methods do not require labeled data. They operate on unlabeled datasets, assuming that anomalies are rare and different from the majority of the data. These methods aim to find patterns or structures in the data that deviate from the norm.\n",
        "- Supervised Anomaly Detection: Supervised methods rely on labeled data, where both normal and anomalous instances are explicitly labeled. The training dataset needs to include examples of both normal and anomalous data to learn the characteristics of anomalies.\n",
        "\n",
        "2. Learning Approach:\n",
        "\n",
        "- Unsupervised Anomaly Detection: Unsupervised methods learn the normal patterns or behaviors from the unlabeled data itself. They typically assume that the majority of the data represents normal instances and aim to detect deviations or outliers that do not conform to those patterns.\n",
        "- Supervised Anomaly Detection: Supervised methods learn the characteristics of anomalies by explicitly training on labeled data. They use the labeled instances to create a model that can distinguish between normal and anomalous instances based on the provided labels.\n",
        "\n",
        "3. Generalization to New Data:\n",
        "\n",
        "- Unsupervised Anomaly Detection: Unsupervised methods focus on learning the underlying structure or representation of the data. They aim to generalize their understanding of normal patterns and identify anomalies that deviate from those patterns, even in unseen data.\n",
        "- Supervised Anomaly Detection: Supervised methods learn from labeled data and aim to classify instances as normal or anomalous based on the provided labels. - The trained model's generalization to new data relies on the assumption that the labeled data represents the full range of anomalies likely to be encountered.\n",
        "\n",
        "4. Applicability and Limitations:\n",
        "\n",
        "- Unsupervised Anomaly Detection: Unsupervised methods are useful when there is limited or no labeled anomaly data available. They can detect unknown or novel anomalies but may have difficulty distinguishing between different types or levels of anomalies.\n",
        "- Supervised Anomaly Detection: Supervised methods are effective when labeled anomaly data is available for training. They can explicitly identify known types of anomalies but may struggle with detecting novel or unseen anomalies that differ significantly from the training data.\n",
        "\n",
        "It's worth noting that some approaches combine elements of both unsupervised and supervised techniques, leveraging limited labeled data to enhance unsupervised anomaly detection or using unsupervised methods to preprocess data before applying supervised methods.\n",
        "\n",
        "The choice between unsupervised and supervised anomaly detection depends on the availability of labeled data, the nature of anomalies, and the specific requirements of the application."
      ],
      "metadata": {
        "id": "XEfU_X0N4NRw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Lkv8gEK4Eor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What are the main categories of anomaly detection algorithms?"
      ],
      "metadata": {
        "id": "i74cqKcr4HlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main categories of anomaly detection algorithms: supervised and unsupervised.\n",
        "\n",
        "- Supervised anomaly detection algorithms use labeled data to learn the normal behavior of the data. This labeled data can be used to train a model that can then be used to identify anomalies in new data.\n",
        "- Unsupervised anomaly detection algorithms do not use labeled data. Instead, they use statistical methods to identify data points that deviate from the norm.\n",
        "\n",
        "Here are some of the most common supervised anomaly detection algorithms:\n",
        "\n",
        "- Isolation forest\n",
        "- Local outlier factor (LOF)\n",
        "- One-class support vector machine (OCSVM)\n",
        "\n",
        "Here are some of the most common unsupervised anomaly detection algorithms:\n",
        "\n",
        "- Gaussian mixture models\n",
        "- K-nearest neighbors (KNN)\n",
        "- Density-based spatial clustering of applications with noise (DBSCAN)\n",
        "\n",
        "The choice of anomaly detection algorithm will depend on the specific application and the characteristics of the data. For example, if the data is labeled, then a supervised anomaly detection algorithm may be a good choice. If the data is unlabeled, then an unsupervised anomaly detection algorithm may be a good choice.\n",
        "\n",
        "Here are some additional factors to consider when choosing an anomaly detection algorithm:\n",
        "\n",
        "- The amount of data. If the amount of data is small, then a simple algorithm may be a good choice. If the amount of data is large, then a more complex algorithm may be necessary.\n",
        "- The computational resources. Some anomaly detection algorithms are more computationally expensive than others. The choice of algorithm will depend on the available computational resources.\n",
        "- The desired level of accuracy. Some anomaly detection algorithms are more accurate than others. The choice of algorithm will depend on the desired level of accuracy.\n",
        "\n",
        "By considering these factors, it is possible to choose an anomaly detection algorithm that is well-suited for the specific application."
      ],
      "metadata": {
        "id": "GNNbTXHM4j4_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSQUWtkx4Iab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. What are the main assumptions made by distance-based anomaly detection methods?"
      ],
      "metadata": {
        "id": "gvRp39-n43VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance-based anomaly detection methods make certain assumptions about the distribution and characteristics of normal instances in the dataset. These assumptions form the basis for identifying anomalies as instances that deviate significantly from the normal patterns. Here are the main assumptions made by distance-based anomaly detection methods:\n",
        "\n",
        "- Normal Instance Density: Distance-based methods assume that normal instances are densely clustered in the feature space. The majority of the data points are expected to exhibit similar patterns or behaviors, forming dense regions or clusters. Anomalies, on the other hand, are expected to be isolated or located in sparse regions with low instance density.\n",
        "\n",
        "- Proximity to Neighbors: Distance-based methods assume that normal instances are closer to their neighboring instances than to anomalies. In other words, normal instances are expected to be similar to their nearest neighbors in terms of feature values or distance metrics. Anomalies, on the contrary, are expected to have larger distances or dissimilarities from their nearest neighbors.\n",
        "\n",
        "- Global vs. Local Structure: Distance-based methods often assume that anomalies exhibit different local or global structural characteristics compared to the normal instances. Anomalies may deviate from the global structure of the data, representing rare events, outliers, or instances with distinct patterns that differ significantly from the majority of the data.\n",
        "\n",
        "- Euclidean Distance or Similarity Metric: Many distance-based methods assume the use of Euclidean distance or a similar distance/similarity metric to measure the proximity or dissimilarity between instances. These methods assume that the Euclidean distance provides a meaningful representation of similarity or dissimilarity in the feature space.\n",
        "\n",
        "- Noisy or Outlying Instances: Distance-based methods assume that anomalies can be considered as noisy or outlying instances that do not conform to the regular patterns of the majority of the data. These methods aim to identify instances that have large distances or dissimilarities from the normal instances.\n",
        "\n",
        "It's important to note that these assumptions may not hold in all scenarios or datasets, and the performance of distance-based anomaly detection methods can be influenced by violations of these assumptions. Therefore, it's crucial to carefully analyze the data and assess the validity of these assumptions before applying distance-based methods for anomaly detection."
      ],
      "metadata": {
        "id": "fyxmBVeo4_Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7sVeQ40344P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. How does the LOF algorithm compute anomaly scores?"
      ],
      "metadata": {
        "id": "1A6d20ga47BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection algorithm that computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors.\n",
        "\n",
        "The LOF algorithm computes anomaly scores using the following steps:\n",
        "\n",
        "- Calculate the k-nearest neighbors of each data point. The k-nearest neighbors of a data point are the k data points that are closest to it.\n",
        "- Calculate the local reachability density (LRD) of each data point. The LRD of a data point is the inverse of the average reachability distance of its k-nearest neighbors. The reachability distance of a data point is the maximum distance between the data point and any of its k-nearest neighbors.\n",
        "- Calculate the local outlier factor (LOF) of each data point. The LOF of a data point is the ratio of the LRD of the data point to the average LRD of its k-nearest neighbors.\n",
        "\n",
        "A data point with a high LOF score is considered to be an outlier. This is because the data point is more isolated than its neighbors, which means that it is less likely to be a normal data point.\n",
        "\n",
        "The LOF algorithm is a powerful tool for anomaly detection. It is relatively simple to implement and it can be used to detect outliers in a variety of data sets. However, the LOF algorithm can be sensitive to the choice of the k parameter. If the k parameter is too small, then the algorithm may not be able to identify enough outliers. If the k parameter is too large, then the algorithm may identify too many false positives.\n",
        "\n",
        "Here are some additional details about the LOF algorithm:\n",
        "\n",
        "- The k parameter is a hyperparameter that controls the number of neighbors that are used to calculate the LRD of each data point.\n",
        "- The LOF algorithm is a non-parametric algorithm, which means that it does not make any assumptions about the distribution of the data.\n",
        "- The LOF algorithm is an unsupervised algorithm, which means that it does not require labeled data to train the model.\n",
        "\n",
        "The LOF algorithm is a versatile tool that can be used to detect outliers in a variety of data sets. It is a good choice for applications where the data is unlabeled and the distribution of the data is unknown."
      ],
      "metadata": {
        "id": "UBtHpJX45JW5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmfb0h1A48Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. What are the key parameters of the Isolation Forest algorithm?"
      ],
      "metadata": {
        "id": "lcFoqwto5bhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Isolation Forest algorithm is an unsupervised anomaly detection algorithm that separates anomalies from normal instances by constructing isolation trees. It utilizes two key parameters: the number of trees (n_estimators) and the subsampling size (max_samples). Here's a description of these parameters:\n",
        "\n",
        "- Number of Trees (n_estimators):\n",
        "The number of trees determines the ensemble size in the Isolation Forest algorithm.\n",
        "Increasing the number of trees improves the algorithm's performance but also increases the computational complexity.\n",
        "A higher number of trees provides a more fine-grained partitioning of the feature space, allowing for better identification of anomalies.\n",
        "The appropriate value for this parameter depends on the dataset size, complexity, and the desired trade-off between computational efficiency and detection accuracy.\n",
        "- Subsampling Size (max_samples):\n",
        "The subsampling size controls the number of instances randomly selected as candidates for splitting at each tree node.\n",
        "A smaller value leads to more random and diverse splits, potentially resulting in better isolation of anomalies.\n",
        "Choosing a larger value can provide a more comprehensive representation of the data but may reduce the effectiveness of isolation for anomalies.\n",
        "The appropriate value for this parameter depends on the dataset size, dimensionality, and the level of noise or outliers present in the data.\n",
        "Additionally, the Isolation Forest algorithm does not explicitly require a predefined threshold to classify instances as anomalies or normal. Instead, the algorithm uses the average path length in the isolation trees to calculate an anomaly score for each instance. Instances with shorter average path lengths are considered anomalies, while instances with longer path lengths are considered normal.\n",
        "\n",
        "It's worth noting that the Isolation Forest algorithm is relatively robust to parameter choices, and the default values often work well in practice. However, fine-tuning these parameters based on the specific characteristics of the dataset may further improve the algorithm's performance."
      ],
      "metadata": {
        "id": "_Nq8S9Pu5hZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFwvTtBf5cY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
      ],
      "metadata": {
        "id": "2sLrVIuc5g8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If a data point has only 2 neighbors of the same class within a radius of 0.5, then its anomaly score using KNN with K=10 would be 0.2. This is because the data point is more isolated than its neighbors, which means that it is less likely to be a normal data point.\n",
        "\n",
        "The anomaly score is calculated as follows:\n",
        "```\n",
        "anomaly_score = 1 - (# of neighbors of the same class) / K\n",
        "```\n",
        "\n",
        "If a data point has only 2 neighbors of the same class within a radius of 0.5, then its anomaly score using KNN with K=10 would be 0.2. This is because the data point is more isolated than its neighbors, which means that it is less likely to be a normal data point.\n",
        "\n",
        "The anomaly score is calculated as follows:\n",
        "\n",
        "Code snippet\n",
        "anomaly_score = 1 - (# of neighbors of the same class) / K\n",
        "Use code with caution. Learn more\n",
        "In this case, the number of neighbors of the same class is 2 and K is 10, so the anomaly score is 0.2.\n",
        "\n",
        "Here is an example of how the anomaly score would be calculated for a data point with 2 neighbors of the same class within a radius of 0.5:\n",
        "```\n",
        "K = 10\n",
        "# of neighbors of the same class = 2\n",
        "anomaly_score = 1 - 2 / 10 = 0.2\n",
        "```\n",
        "An anomaly score of 0.2 is considered to be a low anomaly score. This means that the data point is not very isolated and is likely to be a normal data point. However, it is important to note that the anomaly score is just a single metric and should not be used in isolation to identify outliers. Other factors, such as the distribution of the data and the choice of the K parameter, should also be considered."
      ],
      "metadata": {
        "id": "5WVr8U9v5zdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XN6WJ_3H5wqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
      ],
      "metadata": {
        "id": "Kmz5rpfq6EiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees is 0.5.\n",
        "\n",
        "The Isolation Forest algorithm works by randomly partitioning the data points into different trees. The average path length of a data point is the number of splits it takes to isolate the data point from the rest of the data. A data point with a shorter average path length is considered to be more anomalous, as it is easier to isolate from the rest of the data.\n",
        "\n",
        "In this case, the data point has an average path length of 5.0, which is lower than the average path length of the trees. This means that the data point is more isolated than the average data point, which means that it is more likely to be an anomaly.\n",
        "\n",
        "The anomaly score is calculated as follows:\n",
        "\n",
        "```\n",
        "anomaly_score = 1 - (average path length of the data point / average path length of the trees)\n",
        "```\n",
        "In this case, the average path length of the data point is 5.0 and the average path length of the trees is 10.0, so the anomaly score is 0.5.\n",
        "\n",
        "An anomaly score of 0.5 is considered to be a medium anomaly score. This means that the data point is somewhat isolated and is likely to be an anomaly. However, it is important to note that the anomaly score is just a single metric and should not be used in isolation to identify outliers. Other factors, such as the distribution of the data and the choice of the number of trees, should also be considered.\n",
        "\n",
        "Here is an example of how the anomaly score would be calculated for a data point with an average path length of 5.0 compared to the average path length of the trees:\n",
        "```\n",
        "average path length of the data point = 5.0\n",
        "average path length of the trees = 10.0\n",
        "anomaly_score = 1 - (5.0 / 10.0) = 0.5\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "r-VKeUHk6IRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKdN6jNe6Heu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}